// Initialize Google Cloud TTS Client
const tts = google.texttospeech('v1beta1');
const client = new tts.TextToSpeechClient({
  credentials: require(`./${GOOGLE_APPLICATION_CREDENTIALS}`),
});

const speakerVoiceMap: { [key: string]: string } = {
  "Joe": "en-US-Wavenet-D",  // Male voice
  "Sarah": "en-US-Wavenet-F",  // Female voice
};

// Initialize Vertex AI
vertexai.init({
  project: GOOGLE_CLOUD_PROJECT,
  location: "us-central1"
});

// ElevenLabs API Configuration
const elevenlabsUrl = "https://api.elevenlabs.io/v1/text-to-speech";
const elevenlabsHeaders = {
  "Accept": "audio/mpeg",
  "Content-Type": "application/json",
  "xi-api-key": ELEVENLABS_API_KEY
};

const speakerVoiceMapElevenLabs: { [key: string]: string } = {
  "Joe": "IKne3meq5aSn9XLyUdCD",  // Male voice ID
  "Sarah": "21m00Tcm4TlvDq8ikWAM"  // Female voice ID
};

// Function to synthesize speech using Google TTS
async function synthesizeSpeech(text: string, speaker: string, index: number): Promise<void> {
  const voiceName = speakerVoiceMap[speaker] || "en-US-Wavenet-D";  // Default to Joe
  const synthesisInput = { text: text };
  const voice = {
    languageCode: 'en-US',
    name: voiceName
  };
  const audioConfig = {
    audioEncoding: 'MP3'
  };

  const [response] = await client.synthesizeSpeech({
    request: {
      input: synthesisInput,
      voice: voice,
      audioConfig: audioConfig
    }
  });

  const filename = path.join(AUDIO_FOLDER, `${index}.mp3`);
  fs.writeFileSync(filename, response.audioContent);
  console.log(`Audio content written to file "${filename}"`);
}

// Function to split text into manageable chunks
function splitTextIntoChunks(text: string, maxChars: number = 4000): string[] {
  const sentences = text.split('. ');
  const chunks: string[] = [];
  let currentChunk: string[] = [];

  for (const sentence of sentences) {
    const newChunk = [...currentChunk, sentence].join('. ') + '.';
    if (newChunk.length <= maxChars) {
      currentChunk.push(sentence);
    } else {
      chunks.push(currentChunk.join('. ') + '.');
      currentChunk = [sentence];
    }
  }

  if (currentChunk.length) {
    chunks.push(currentChunk.join('. ') + '.');
  }

  return chunks;
}

// Function to clean generated text
function cleanGeneratedText(rawText: string): any[] {
  try {
    const data = JSON.parse(rawText);
    const conversation: { speaker: string, text: string }[] = [];
    if (data.podcastConversation) {
      for (const entry of data.podcastConversation) {
        const speaker = entry.speaker || "Unknown";
        const dialogue = entry.dialogue.trim();
        if (speaker && dialogue) {
          conversation.push({ speaker, text: dialogue });
        }
      }
    }
    return conversation;
  } catch (error) {
    console.error("Error parsing text:", error);
    return [];
  }
}

// Function to merge audio files into a single podcast
function mergeAudios(audioFolder: string, outputFile: string): void {
  const combined = new AudioSegment();
  const introAudio = AudioSegment.fromFile('podcast.mp3');
  combined.add(introAudio);

  const audioFiles = fs.readdirSync(audioFolder)
    .filter(file => file.endsWith('.mp3'))
    .sort((a, b) => parseInt(a) - parseInt(b));  // Sort numerically

  for (const filename of audioFiles) {
    const audioPath = path.join(audioFolder, filename);
    console.log(`Processing: ${audioPath}`);
    const audio = AudioSegment.fromFile(audioPath);
    combined.add(audio);
  }

  combined.export(outputFile, 'mp3');
  console.log(`Merged audio saved as ${outputFile}`);
}

// Function to generate a conversation using Vertex AI
async function generateConversation(articleText: string): Promise<any[]> {
  const textChunks = splitTextIntoChunks(articleText);
  let allConversations: any[] = [];
  let lastResponse: string = "";
  let speakerIndex = 0;
  const speakers = ["Joe", "Sarah"];

  for (const [index, chunk] of textChunks.entries()) {
    let systemPrompt = "";

    if (index === 0) {
      systemPrompt = systemPrompt_1;
    } else if (index === textChunks.length - 1 && lastResponse) {
      systemPrompt = systemPrompt_3;
    } else {
      systemPrompt = systemPrompt_2;
    }

    const prompt = `${systemPrompt}\n\n${speakers[speakerIndex]}: ${chunk}\n\n${speakers[(speakerIndex + 1) % 2]}:`;

    const response = await vertexai.generativeModels.gemini_1_5_flash_002.generateContent({
      prompts: [prompt],
      generationConfig: generationConfig,
      stream: false
    });

    const rawText = response.candidates[0].content.parts[0].text;
    const conversationChunk = cleanGeneratedText(rawText);

    allConversations = [...allConversations, ...conversationChunk];
    if (conversationChunk.length) {
      lastResponse = conversationChunk[conversationChunk.length - 1].text;
      speakerIndex = (speakerIndex + 1) % 2;
    }
  }

  return allConversations;
}

// Extract text from PDF
function extractTextFromPdf(pdfFile: string): string {
  const pdfBuffer = fs.readFileSync(pdfFile);
  const data = PyPDF2.PdfReader(pdfBuffer);
  let text = '';
  for (const page of data.pages) {
    text += page.extractText();
  }
  return text;
}
