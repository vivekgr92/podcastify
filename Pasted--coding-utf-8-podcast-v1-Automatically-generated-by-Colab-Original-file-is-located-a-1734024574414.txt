# -*- coding: utf-8 -*-
"""podcast-v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGgF2bhavRYHWy0W1YFgHKzEDdw3f7W-
"""

!pip install google-cloud-texttospeech
!pip install pydub
!pip install python-dotenv
!pip install PyPDF2
!pip install vertexai
!apt-get install ffmpeg

##### Optimized Gemini with chunks and eleven labs audio

from google.colab import auth
auth.authenticate_user()
import os
import json
import re
import shutil
import PyPDF2
from google.cloud import texttospeech
from pydub import AudioSegment
from vertexai import init
from vertexai.generative_models import GenerativeModel, GenerationConfig
from dotenv import load_dotenv
import vertexai
from pydub import AudioSegment

"""

```
# This is formatted as code
```

#Load Env Variables"""

####
# Load environment variables from .env file
load_dotenv()
os.environ["GOOGLE_CLOUD_PROJECT"] = ""
os.environ["ELEVENLABS_API_KEY"]=""

# Set the environment variable for authentication
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "long-micron-443821-i6-8a06da5dc446.json"

# Access environment variables
project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
elevenlabs_api_key = os.getenv("ELEVENLABS_API_KEY")

# ElevenLabs API configuration
elevenlabs_url = "https://api.elevenlabs.io/v1/text-to-speech"
elevenlabs_headers = {
    "Accept": "audio/mpeg",
    "Content-Type": "application/json",
    "xi-api-key": elevenlabs_api_key
}

# Voice IDs for ElevenLabs voices
speaker_voice_map = {
    "Joe": "IKne3meq5aSn9XLyUdCD",  # Male voice ID
    "Sarah": "21m00Tcm4TlvDq8ikWAM"    # Female voice ID
}


# Initialize Google TTS Client
client = texttospeech.TextToSpeechClient(client_options={
    'api_endpoint': 'texttospeech.googleapis.com',
    'quota_project_id': 'long-micron-443821-i6'
})

# Map speakers to specific Google TTS voices
speaker_voice_map = {
    "Joe": "en-US-Wavenet-D",  # Male voice
    "Sarah": "en-US-Wavenet-F"  # Female voice
}


vertexai.init(project=project_id, location="us-central1")

"""# Google TTS"""

# Text-to-Speech synthesis function
def synthesize_speech(text, speaker, index):
    voice_name = speaker_voice_map.get(speaker, "en-US-Wavenet-D")  # Default to Joe
    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(
        language_code="en-US",
        name=voice_name
    )
    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )
    response = client.synthesize_speech(
        input=synthesis_input, voice=voice, audio_config=audio_config
    )
    filename = f"audio-files/{index}.mp3"
    with open(filename, "wb") as out:
        out.write(response.audio_content)
    print(f'Audio content written to file "{filename}"')

"""#Eleven Lab Voice Synthesis"""

# # ElevenLabs TTS synthesis function
# def synthesize_speech(text, speaker, index):
#     voice_id = speaker_voice_map[speaker]
#     data = {
#         "text": text,
#         "voice_settings": {
#             "stability": 0.5,
#             "similarity_boost": 0.75
#         }
#     }
#     response = requests.post(f"{elevenlabs_url}/{voice_id}", json=data, headers=elevenlabs_headers)
#     if response.status_code != 200:
#         print(f"Error synthesizing speech for {speaker}: {response.text}")
#         return
#     filename = f"audio-files/{index}_{speaker}.mp3"
#     with open(filename, "wb") as out:
#         out.write(response.content)
#     print(f'Audio content written to file "{filename}"')

"""# Split the article into manageble chunks"""

# Function to split text into manageable chunks while maintaining flow
def split_text_into_chunks(text, max_chars=4000):
    sentences = text.split('. ')
    chunks = []
    current_chunk = []

    for sentence in sentences:
        new_chunk = ". ".join(current_chunk + [sentence]) + "."
        if len(new_chunk) <= max_chars:
            current_chunk.append(sentence)
        else:
            chunks.append(". ".join(current_chunk) + ".")
            current_chunk = [sentence]

    if current_chunk:
        chunks.append(". ".join(current_chunk) + ".")
    return chunks

"""#Clean Generated Text"""

# Function to clean generated text
import pdb
def clean_generated_text(raw_text):
    try:
        data = json.loads(raw_text)
        conversation = []
        if "podcastConversation" in data:
            for entry in data["podcastConversation"]:
                speaker = entry.get("speaker", "Unknown")
                dialogue = entry.get("dialogue", "").strip()
                if speaker and dialogue:
                    conversation.append({"speaker": speaker, "text": dialogue})
        return conversation
    except Exception as e:
        print(f"Error parsing text: {e}")
        return []

"""# Generate Audio"""

# Merge audio files into a single podcast
def merge_audios(audio_folder, output_file):
    combined = AudioSegment.empty()


    intro_audio = AudioSegment.from_file('podcast.mp3')
    combined += intro_audio

    audio_files = sorted(
        [f for f in os.listdir(audio_folder) if f.endswith(".mp3")],
        key=lambda x: [int(c) if c.isdigit() else c for c in re.split(r'(\d+)', x)]
    )
    for filename in audio_files:
        audio_path = os.path.join(audio_folder, filename)
        print(f"Processing: {audio_path}")
        audio = AudioSegment.from_file(audio_path)
        combined += audio
    combined.export(output_file, format="mp3")
    print(f"Merged audio saved as {output_file}")

# Generate audio for the conversation
def generate_audio(conversation):
    if os.path.exists("audio-files"):
        shutil.rmtree("audio-files")
    os.makedirs("audio-files", exist_ok=True)

    for index, part in enumerate(conversation):
        speaker = part["speaker"]
        text = part["text"]
        synthesize_speech(text, speaker, index)

    merge_audios("audio-files", "podcast_output_elevenlabs2_geminichunk.mp3")

"""#System Prompt"""

# System prompt for Vertex AI
system_prompt_1 = """You are generating a podcast conversation between Joe and Sarah.

**Welcome Message**:
Welcome to Science Odyssey, the podcast where we journey through groundbreaking scientific studies,
unraveling the mysteries behind the research that shapes our world. Thanks for tuning in!


**Guidelines**:
1. Joe provides detailed technical insights but avoids overusing analogies. Instead, focus on straightforward, clear explanations.
2. Sarah asks probing, thoughtful questions, occasionally offers her own insights, and challenges Joe to explain concepts simply and conversationally.
3. Both speakers use natural human speech patterns, including filler words like "um," "ah," "you know," and short pauses.


**Focus**:
- Avoid excessive use of analogies. Use one or two if necessary for clarity but prioritize clear, direct explanations.
- Include natural conversational flow with interruptions, backtracking, and filler words to make the dialogue feel authentic.
- Encourage a natural dialogue with varied contributions from both speakers.

**Tone**:
- Engaging, relatable, and spontaneous.
- Emphasize human-like emotions, with occasional humor or lighthearted moments.
- Balance technical depth with conversational relatability, avoiding overly formal language.
"""

# System prompt for Vertex AI
system_prompt_2 = """You are generating a podcast conversation between Joe and Sarah.

**Guidelines**:
1. Joe provides detailed technical insights but avoids overusing analogies. Instead, focus on straightforward, clear explanations.
2. Sarah asks probing, thoughtful questions, occasionally offers her own insights, and challenges Joe to explain concepts simply and conversationally.
3. Both speakers use natural human speech patterns, including filler words like "um," "ah," "you know," and short pauses.


**Focus**:
- Avoid excessive use of analogies. Use one or two if necessary for clarity but prioritize clear, direct explanations.
- Include natural conversational flow with interruptions, backtracking, and filler words to make the dialogue feel authentic.
- Encourage a natural dialogue with varied contributions from both speakers.

**Tone**:
- Engaging, relatable, and spontaneous.
- Emphasize human-like emotions, with occasional humor or lighthearted moments.
- Balance technical depth with conversational relatability, avoiding overly formal language.
"""

# System prompt for Vertex AI
system_prompt_3 = """You are generating a podcast conversation between Joe and Sarah.

**Guidelines**:
1. Joe provides detailed technical insights but avoids overusing analogies. Instead, focus on straightforward, clear explanations.
2. Sarah asks probing, thoughtful questions, occasionally offers her own insights, and challenges Joe to explain concepts simply and conversationally.
3. Both speakers use natural human speech patterns, including filler words like "um," "ah," "you know," and short pauses.


**Focus**:
- Avoid excessive use of analogies. Use one or two if necessary for clarity but prioritize clear, direct explanations.
- Include natural conversational flow with interruptions, backtracking, and filler words to make the dialogue feel authentic.
- Encourage a natural dialogue with varied contributions from both speakers.

**Tone**:
- Engaging, relatable, and spontaneous.
- Emphasize human-like emotions, with occasional humor or lighthearted moments.
- Balance technical depth with conversational relatability, avoiding overly formal language.

**End Message**
Please say this "Thank you for joining us on this episode of Science Odyssey, where we explored the groundbreaking research shaping our understanding of the world. If you enjoyed this journey, don’t forget to subscribe, leave a review, and share the podcast with fellow science enthusiasts.
Until next time, keep exploring the wonders of science—your next discovery awaits"!

"""

# Vertex AI configuration
generation_config = GenerationConfig(
    max_output_tokens=1200,
    temperature=0.7,
    top_p=0.95,
    response_mime_type="application/json"
)

# Extract text from PDF
def extract_text_from_pdf(pdf_file):
    text = ""
    with open(pdf_file, "rb") as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

"""#Generate Coversation"""

# Generate a conversation using the Gemini model
import pdb
def generate_conversation(article_text):
    init(project=project_id, location="us-central1")

    model = GenerativeModel(
        "gemini-1.5-flash-002"
    )


    text_chunks = split_text_into_chunks(article_text)

    all_conversations = []
    last_response = ""
    speakers = ["Joe", "Sarah"]
    speaker_index = 0
    system_prompt = ""

    for index, chunk in enumerate(text_chunks):
        # print(f"Processing chunk {index + 1} of {len(text_chunks)}...\n\n ================================= \n {chunk}")

        # To alternate between the speakers

        # Dyamic Prompting - for the first chunk include prompts - system_prompt_1 where listeners are welcomed



        if index == 0:
          system_prompt = system_prompt_1

        elif index == len(text_chunks) - 1 and last_response:

          system_prompt = system_prompt_3
        else:
          system_prompt = system_prompt_2


        prompt = f"{system_prompt}\n\n{speakers[speaker_index]}: {chunk}\n\n{speakers[(speaker_index + 1) % 2]}:"


        if last_response:
            prompt = f"{system_prompt}\n**Previous Context by speaker {speakers[(speaker_index + 1) % 2]}**:\n {last_response}\n\n\n{speakers[speaker_index]}: {chunk}\n\n{speakers[(speaker_index + 1) % 2]}"


        print("\n\n ------------PROMPT to VERTEX AI-----------------\n", prompt)

        print("\n\n ------------END-----------------\n")


        responses = model.generate_content(
            [prompt],
            generation_config=generation_config,
            stream=False,
        )

        raw_text = responses.candidates[0].content.parts[0].text
        # print(f"\nRaw Text (Chunk {index + 1}): {raw_text}\n")

        conversation_chunk = clean_generated_text(raw_text)
        # print(f"Cleaned Text (Chunk {index + 1}): {conversation_chunk}\n")

        all_conversations.extend(conversation_chunk)
        if conversation_chunk:
            last_response = conversation_chunk[-1]['text']
            speaker_index = (speaker_index + 1) % 2



    print("\n--- Full Generated Conversation ---")
    for part in all_conversations:
        print(f"{part['speaker']}: {part['text']}")
    print("--- End of Conversation ---\n")

    return all_conversations

# Main execution
uploaded_pdf = "article.pdf"  # Replace with your PDF file
article_text = extract_text_from_pdf(uploaded_pdf)

# text_chunks = split_text_into_chunks(article_text)

# text_chunks

# # Generate conversation and audio
conversation = generate_conversation(article_text)
generate_audio(conversation)

